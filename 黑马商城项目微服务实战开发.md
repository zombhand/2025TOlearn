# 黑马商城项目微服务实战开发

## 导入数据库

在黑马中导入数据使用docker进行自动化导入

```bash
docker run -d \
  --name mysql \
  -p 3307:3306 \
  -e TZ=Asia/Shanghai \
  -e MYSQL_ROOT_PASSWORD=123 \
  -v /root/mysql/data:/var/lib/mysql \
  -v /root/mysql/conf:/etc/mysql.conf.d \
  -v /root/mysql/init:/docker-entrypoint-initdb.d \
  --network hm-net \
  mysql
```

## 拆分服务

首先在父工程下新建个模块

然后把需要的依赖给引入

在新的工程的启动项上进行个注解

```java
@MapperScan("com.heima.item.mapper")
@SpringBootApplication
public class ItemServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(ItemServiceApplication.class, args);
    }

}
```

@MapperScan

作用：指定要变成实现类的接口所在的包，然后包下面的所有接口在编译之后都会生成相应的实现类

添加位置：是在SpringBoot启动类上面添加



#### 在配置中

需要注意的点:

1. 每个微服务名字各异
2. logging配置中
   1. level是输出级别
   2. fill-path 是文件的输出位置，保证每个服务的输出位置不同

```yaml
server:
  port: 8081
# 要给微服务起个名字
spring:
  application:
    name: item-service # 微服务名称
  profiles:
    active: dev
  datasource:
    url: jdbc:mysql://${hm.db.host}:3307/hmall?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
    driver-class-name: com.mysql.cj.jdbc.Driver
    username: root
    password: ${hm.db.pw}
mybatis-plus:
  configuration:
    default-enum-type-handler: com.baomidou.mybatisplus.core.handlers.MybatisEnumTypeHandler
  global-config:
    db-config:
      update-strategy: not_null
      id-type: auto
logging:
  level:
    com.hmall: debug
  pattern:
    dateformat: HH:mm:ss:SSS
  file:
    path: "logs/${spring.application.name}"
knife4j:
  enable: true
  openapi:
    title: 黑马商城商品管理文档
    description: "黑马商城商品管理文档"
    email: zhanghuyi@itcast.cn
    concat: 虎哥
    url: https://www.itcast.cn
    version: v1.0.0
    group:
      default:
        group-name: default
        api-rule: package
        api-rule-resources:
          - com.heima.item.controller

```

## 注册中心

![image-20250116172100706](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250116172100706.png)

**服务治理中的三个角色分别是什么？**

- 服务提供者：暴露服务接口，供其它服务调用
- 服务消费者：调用其它服务提供的接口
- 注册中心：记录并监控微服务各实例状态，推送服务变更信息

**消费者如何知道提供者的地址？**

- 服务提供者会在启动时注册自己信息到注册中心，消费者可以从注册中心订阅和拉取服务信息

**消费者如何得知服务状态变更？**

- 服务提供者通过心跳机制向注册中心报告自己的健康状态，当心跳异常时注册中心会将异常服务剔除，并通知订阅了该服务的消费者

**当提供者有多个实例时，消费者该选择哪一个？**

- 消费者可以通过负载均衡算法，从多个实例中选择一个

**服务发现**

## 最佳实现

![image-20250116203538032](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250116203538032.png)

![image-20250116203654016](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250116203654016.png)



当定义的FeignClient不在SpringBootApplication的扫描包范围时，这些FeignClient无法使用。有两种方式解决：

方法一：指定FeignClient所在包

```java
@EnableFeignClients(basePackages = "com.hmall.api.clients")
```

方法二：指定FeignClient字节码

```java
@EnableFeignClients(clients = {UserClient.class})
```

## 网关

1. 创建新模块

2. 引入网关依赖

3. 编写启动类

4. 配置路由规则

   ```yaml
   spring:
   	cloud:
   		gataway:
   			routes:
   				- id : item # 路由规则id，自定义，唯一
   				  uri: lb://item-service # 路由目标微服务，lb代表负载均衡
   				  predicates: # 路由断言，判断请求是否符合规则，符合则路由到目标
   				  	- Paht=/items/** # 以请求路径做判断，以/items开头则符合
   				- id: xx
   			      uri: lb://xx-service
   			      predicates:
   			      	- Path=/xx/**
   ```

## 网关登录校验

![image-20250118012006394](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118012006394.png)

- 如何在网关转发之前做登录校验?
- 网关如何将用户信息传递给微服务？
- 如何在微服务之间传递用户信息？

## 自定义过滤器

网关过滤器有两种，分别是：

- GatewayFilter:路由过滤器，作用于任意指定的路由；默认不生效，要配置到路由后生效。
- GlobalFilter：全局过滤器，作用范围是所有路由;声明后自动生效。

两种过滤器的**过滤方法**签名完全一致：

![image-20250118012537329](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118012537329.png)

### 自定义过滤器GlobalFilter

自定义GlobalFilter比较简单，直接实现GlobalFilter接口即可：

```java
@Commpont
public class MyGlobalFilter implements GlobalFilter{
    
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        // 1.获取请求
        ServerHttpRequest request = exchange.getRequest();
        // 2.过滤器业务处理
        System.out.println("GlobalFilter pre阶段 执行了...");
        // 3.放行
        return chain.filter(exchange);
    }
    
    @Override
    public int getOrder() { // 用于确定优先级，返回数字越小优先级越高
        return 0; 
    }
}
```

## 网关传递用户到微服务



现在，网关已经可以完成登录校验并获取登录用户身份信息。但是当网关将请求转发到微服务时，微服务又该如何获取用户身份呢？

由于网关发送请求到微服务依然采用的是`Http`请求，因此我们可以将用户信息以请求头的方式传递到下游微服务。然后微服务可以从请求头中获取登录用户信息。考虑到微服务内部可能很多地方都需要用到登录用户信息，因此我们可以利用SpringMVC的拦截器来实现登录用户信息获取，并存入ThreadLocal，方便后续使用。

据图流程图如下：

![img](https://b11et3un53m.feishu.cn/space/api/box/stream/download/asynccode/?code=M2FjYTQ4MjRlOTVmYzhkNTBhNzA1NTE4YjRmYmNlODVfUDBqenRVSHc4UlhLT0hkVVlTNGUyR1Iwc3Y2TDVRbWhfVG9rZW46S2h4RWI2cmpCb05yVkN4c2VoTGNGUExFblNkXzE3MzcyMDY2NzY6MTczNzIxMDI3Nl9WNA)

接下来要做的事情有：

- 改造网关过滤器，在获取用户信息后保存到请求头，转发到下游微服务
- 编写微服务拦截器，拦截请求获取用户信息，保存到ThreadLocal后放行

1. 保存用户到请求头

   首先，修改登录校验拦截器的处理逻辑，保存用户信息到请求头中：

   ```java
   @Override
   public Mono<Void> filter (serverWebExchange exchange, GatewayFilterChain chain) {
       // 1.获取request
       ServerHttpRequest request = exchange.getRequest();
       // 2.判断是否需要做登录拦截
       if(isExclude(request.getPath().toString())){...}
       // 3.获取token
       String token = null;
       List<String> headers = request.getHeaders().get("authorization");
       if (headers != null && !headers.isEmpty()){...}
       // 4.校验并解析token
       Long userId = null;
       try {
           userId = jwtTool.parseToken(token);
       } catch (UnauthorizedException e)
       {...}
       // 5.传递用户信息
       String userInfo = userId.toString();
       ServerWebExchange ex = exchange.mutate()
           .request(b -> b.header("user-info", userInfo))
           .builf();
       // 6.放行
       return chain.filter(ex);
   }
   ```

2. 拦截器获取用户

   使用common包内的ThreadLocal工具

   接下来，只需要编写拦截器，获取用户信息并保存到UserContext，然后放行即可。

   由于每个微服务都有获取登录用户的需求，因此拦截器我们直接写在common中，并写好自动装配。这样微服务只需要接入common就可以直接具备拦截器功能，无需重复编写。

   在common模块内定义一个拦截器:

   ```java
   @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
       // 1.获取请求头中的用户信息
       String userInfo = request.getHeader("user-info");
       // 2.判断是否为空
       if (StrUtil.isNotBlank(userInfo)) {
           // 不为空，保存到ThreadLocal
           	UserContext.setUser(Long.valueOf(userInfo));
       }
       // 3.放行
       return true;
   }
   
   @Override
   public void afterCompletion(HttpServletRequest request, HttpServletResponse response){
       // 移除用户
       UserContext.removerUser();
   }
   ```

   接着编写SpringMVC的配置类，配置登录拦截器：

   ```java
   @Configuration
   @ConditionalOnClass(DispathcherServlet.class) // 排除网关避免冲突
   public class MvcConfig implements WebMvcConfigurer {
       @Override
       public void addInterceptors(InterceptorRegistry registry) {
           registry.addInterceptor(new UserInfoInterceptor());
       }
   }
   ```

   不过，需要注意的是，这个配置类默认是不会生效的，因为它所在的包是`com.hmall.common.config`，与其它微服务的扫描包不一致，无法被扫描到，因此无法生效。

   基于SpringBoot的自动装配原理，我们要将其添加到`resources`目录下的`META-INF/spring.factories`文件中：

   ```properties
   org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
     com.hmall.common.config.MyBatisConfig,\
     com.hmall.common.config.MvcConfig
   ```

   

![image-20250118220725647](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118220725647.png)

## 分布式事务



# 实用的点

### 工具类

其中的BeanUtil 很好用

```xml
<dependency>
            <groupId>cn.hutool</groupId>
            <artifactId>hutool-core</artifactId>
            <version>5.8.35</version>
</dependency>
```

### mybatisplus实用插件使用

![image-20250112174704016](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250112174704016.png)

### 查询集合时判空使用

```java
CollUtil.isEmpty(list);
// 为空的话返回一个
Collections.emptyList();
```



```java
// 已知ids获取对应用户
List<User> userList = listByIds(ids);
// 1获取用户id集合
List<Long> idList = userList.lambda().map(User::getId).collect(Collectors.toList());

// 2根据用户id查询地址
List<Address> address = Db.lambdaQuery(Address.class).in(Address::getUserId, userIds).list();
// 2.1获取用户id集合
List<Long> idList = userList.stream().map(User::getId).toList();
// 2.2根据用户id查询地址
List<Address> addressList = Db.lambdaQuery(Address.class).in(Address::getUserId, idList).lsit();
// 2.3转换地址VO
List<AddressVO> addressVOS = BeanUtil.copyToList(addressList, AddressVO.class);
// 2.4用户地址集合分组处理，相同用户的放入一个集合（组）中
Map<Long, List<AddressVO>> addressMap = new HashMap<>(0);
if(CollUtil.isNotEmpty(addressList)){
    addressMap = addressMap.stream().collect(Collectors.groupingBy(AddressVO::getUserId));
}
// 3.转换VO返回
List<UserVO> userVOList = new ArrayList<>(userList.size());
for (User user : userList) {
    // 3.1 转换UserPO为 VO
    UserVO vo = BeanUtil.copyProperties(user, UserVO.class);
    userVOlist.add(vo);
    // 3.2转换地址VO
    vo.setAddressVOList(addressMap.get(user.getId()));
}
```

### 状态字段的ENUM化

使用enmu类

```java
@Getter
public enum UserStatus {
    NORMAL(1, "正常"),
    FROZEN(2, "冻结"),
    ,
    @EnumValue
    private final int value;
    private final String desc;
    
    UserStatus(int value, String desc) {
        this.value = value;
        this.desc = desc;
    }
}
```

### JSON数据处理

对JSON化的数据项单独建个类

```java
@Data
@NoArgsConstructor
@AllArgsConstructor(staticName = "of")
public class UserInfo{
    private Integer age;
    private String intro;
    private String gender;
}
```

对实体类的JSON项进行注解

```java
@Data
@TableName(value = "user", autoResultMap = true)
public class User{
    /**
    *详细信息
    */
    @TableField(typeHandler = JasksonTypeHandler.class)
    private UserInfo info;
}
```

### 通用分页实体和MP转换

通用的PageQuery以UserQuery继承为例

```java
@EqualsAndHashCode(callSuper = true)
@Data
@ApiModel(description = "用户查询条件实体")
public class UserQuery extends PageQuery{
    @ApiModelProperty("用户关键字")
    private String name;
    @ApiModelProperty("用户状态：1-正常，2-冻结")
    private Integer status;
    @ApiModelProperty("余额最小值")
    private Integer minBalance;
    @ApiModelProperty("余额最大值")
    private Integer maxBalance;
}
```

```java
@Data
@ApiModel(description = "分页查询实体")
public class PageQuery{
    
    @ApiModelProperty("页码")
    private Integer pageNo = 1;
    @ApiModelProperty("页码")
    private Integer pageSize = 5;
    @ApiModelProperty("排序字段")
    private String sortBy;
    @ApiModelProperty("是否升序")
    private Boolean isAsc = true;
    
    public <T> Page<T> toMpPage(OrederItem ... itmes) {
        // 1.分页条件
        Page<T> page = Page.of(pageNo, pageSize);
        // 2.排序条件
        if(StrUtil.isNotBlank(sortBy)){
            //不为空
            page.addOrder(new OrderItem(sortBy, isAsc));
        }else if(items != null){
            // 为空，默认排序
            page.addOrder(items);
        }
        return page;
    }
    
    private <T> Page<T> toMpPage(String defaultSortBy, Boolean defaultAsc) {
        return toMpPage(new OrderItem(defaultSortBy, defaultAsc));
    }
    public <T> Page<T> toMpPageDefaultSortByCreateTime(){
        return toMpPage(new OrderItem("create_time", false));
    }
    public <T> Page<T> toMpPageDefaultSortByUpdateTime(){
        return toMpPage(new OrderItem("update_time", false));
    }
}
```

当PO和VO属性名相同时

```java
public static <PO, VO> pageDTO<VO> of(Page<PO> p, Class<VO> clazz){ // 泛型只是占位符不能获取它的字节符，所以需要调用者传入
    PageDTO<VO> dto = new PageDTO<>();
    // 1.总条数
    dto.setTotal(p.getTotal());
    // 2.总页数
	dto.setPages(p.getPages());
    // 3.当前页数据
    List<PO> records = p.getRecords();
    if (CollUtil.isEmpty(records)) {
        dto.setList(Collections.emptyList());
        return dto;
    }
    // 4.拷贝user的VO
    dto.setList(BeanUtil.copyToList(records, clazz));
    // 5.返回
    return dto;
}
```

当PO和VO属性名不同时

```java
public static <PO, VO> pageDTO<VO> of(Page<PO> p, Class<VO> clazz){ // 泛型只是占位符不能获取它的字节符，所以需要调用者传入
    PageDTO<VO> dto = new PageDTO<>();    
    // 1.总条数   
    dto.setTotal(p.getTotal());    
    // 2.总页数    
    dto.setPages(p.getPages());    
    // 3.当前页数据 
    List<PO> records = p.getRecords();    
    if (CollUtil.isEmpty(records)) {        dto.setList(Collections.emptyList());        return dto;    }    // 4.拷贝user的VO    
    dto.setList(records.stream().map(convertor).collect(Collectors.toList()));    
    // 5.返回    
    return dto;
}
```

调用案例

```java
@Override
Public PageDTO<UserVO> queryUsersPage(UserQuery query) {
    String name = query.getName();
    Integer status = query.getStatus();
    // 1.构建分页条件
    Page<User> page = query.toMpPageDefaultSortByUpdateTime();
    // 2.分页查询
    Page<User> p = lambdaQuery()
        			.like(name != null, User::getUsername, name)
        			.eq(status != null, User::getStatus, status)
        			.page(page);
    
    // 3.封装VO结果
    return PageDTO.of(p, user -> {
        // 1.拷贝基础属性
        UserVO vo = BeanUtil.copyProperties(user, UserVO.class);
        // 2.处理特殊逻辑
        vo.setUsername(vo.getUsername().subString(0, vo.getUsername().length() - 2) + "**");
        return vo;
    });
}
```

### 导入Bean

在类上加上@RequiredArgsConstructor

表示这个类在加载时要导入声明了final的属性

```java
@Service
@RequiredArgsConstructor
public class CartServiceImpl extends ServiceImpl<CartMapper, Cart> implements ICartService {
    
    private final RestTemplate  restTemplate;
}
```



# docker

```bash
docker run -d \
	--name mysql \
	-p 3306:3306 \
	-e TZ=Asia/Shanghai \
	-e MYSQL_ROOT_PASSWORD=123456 \
	mysql
```

- docker run: 创建并运行一个容器，-d 是让容器在后台运行
- --name mysql ：给容器起个名字，必须唯一
- -p 3306:3306 ：设置端口映射
- -e KEY=VALUE：是设置环境变量
- mysql：指定运行的镜像的名字

# 微服务

### 服务拆分原则

从拆分目标来说，要做到：

- 高内聚：每个微服务的职责要尽量单一，包含的业务相互关联度高、完整度高。
- 低耦合：每个微服务的功能要相对独立，尽量减少对其它微服务的依赖

拆分方式，两种方法：

- 纵向拆分：按照业务模块来拆分
- 横向拆分：抽取公共服务，提高复用性。

拆分后碰到的第一个问题是什么，如何解决？

- 拆分后，某些数据在不同服务，无法直接调用本地方法查询数据
- 利用RestTemplate发送Http请求，实现远程调用

# 远程调用

Spring给我们提供了一个RestTemplate工具，可以方便的实现Http请求的发送。使用步骤如下：

1. 注入RestTemplate到Spring容器

   ```java
   @Bean
   public RestTemplate restTemplate(){
       return new RestTemplate();
   }
   ```

2. 发起远程调用

   ```java
   public <T> ResponseEntity<T> exchange(
       String url, // 请求路径 -"http://localhost:8081/items?id={id}"
       HttpMethod method, // 请求方式	HttpMethod.GET
       @Nullable HttpEntity<?> requestEntity, // 请求实体，可以为空
       Class<T> responseType, // 返回值类型	User.class
       Map<String, ?> uriVariables // 请求参数	Map.of("id", "1")
   )
   ```

   实际调用案例

   ```java
   private void handleCartItems(List<CartVO> vos) {
       // 1.获取商品id
       Set<Long> itemIds = vos.stream().map(CartVO::getItemId).collect(Collectors.toSet());
       // 2.查询商品
       // List<ItemDTO> items= itemService.queryItemByIds(itemIds);
       // 2.1 利用RestTemplate 发起http请求，得到http的响应
       ResponseEntity<List<ItemDTO>> response = restTemplate.exchange(
       	"http://localhost:8081/items?ids={ids}",
           HttpMethod.GET,
           null,
           new ParameterizedTypeReference<List<ItemDTO>>(){},
           Map.of("ids",CollUtil.join(itemIds, ","))
       )
   }
   ```

   其中有几个点需要注意

   1. 在返回类型中由于比较复杂是一个LIST《ITEMDTO》的形式所以不能通过直接传一个class字节码来确定返回类型，所以使用new 一个类来解决
   2. 在Map的值中，由于对于Ids是一个字符串，不能用集合来传入，使用了hutool的工具CollUtil.join将集合内的参数以指定的字符连接起来，

# Nacos注册中心

Nacos是目前国内企业中占比最多的注册中心组件。它是阿里巴巴的产品，目前已经加入SpringCloudAlibaba中。

使用步骤

1. 导入nacos的数据库

2. 将`nacos/custom.env`的配置放入root目录

   ```
   PREFER_HOST_MODE=hostname
   MODE=standalone
   SPRING_DATASOURCE_PLATFORM=mysql
   MYSQL_SERVICE_HOST=	//改成自己数据库的地址
   MYSQL_SERVICE_DB_NAME=nacos
   MYSQL_SERVICE_PORT=3307	//改成自己数据库的端口
   MYSQL_SERVICE_USER=root	//改成自己数据库的姓名
   MYSQL_SERVICE_PASSWORD=123	//改成自己数据库的密码
   MYSQL_SERVICE_DB_PARAM=characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=Asia/Shanghai
   ```

3. 使用docker指令进行容器配置运行

   ```bash
   docker run -d \
   --name nacos \
   --env-file ./nacos/custom.env \
   -p 8848:8848 \
   -p 9848:9848 \
   -p 9849:9849 \
   --restart=always \
   nacos/nacos-server:v2.1.0-slim
   ```

   8848 是用于客户端与服务通信的主要端口。

   9848 是 gRPC 端口，用于与 Nacos 的 gRPC 通信（如果需要）。

   9849 是 Raft 端口，用于 Nacos 集群中节点之间的通信（如果运行集群模式）。

   启动完成后，访问下面地址：http://192.168.150.101:8848/nacos/，注意将`192.168.150.101`替换为你自己的虚拟机IP地址。

   首次访问会跳转到登录页，**账号密码都是nacos**

4. **服务注册**

   1. 引入依赖

      ```xml
      <!--nacos 服务注册发现-->
      <dependency>
      	<groupId>com.alibaba.cloud</groupId>
          <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
      </dependency>
      ```

   2. 配置Nacos地址

      ```yaml
      spring:
      	application:
      		name: item-service # 服务名称
      	cloud:
      		nacos:
      			server-addr: 192.168.150.101:8848 # nacos地址
      ```

5. **服务发现**

   **消费者**需要连接nacos以拉取和订阅服务，因此服务发现的前两步与服务注册是一样的，后面再加上服务调用即可：

   1. 引入 nacos discovery依赖

   2. 配置nacos地址

   3. 服务发现

      ```java
      private final DiscoveryClient discoveryClient;
      
      private void handleCartItems(List<CartVO> vos) {
          // 1.根据服务名称，拉取服务的实例列表
          List<ServiceInstance> instances = discoveryClient.getInstances("item-service");
          // 2.负载均衡，挑选一个实例
          ServiceInstance instance = instances.get(RandomUtil.randomInt(instances.size()));//传入一个负载均衡算法
          // 3.获取实例的IP和端口
          URI uri = instance.getUri();
          // ... 略
      }
      ```

## OpenFeign

OpenFeign是一个声明式的http客户端，是SpringCloud在Eureka公司开源的Feign基础上改造而来

原始的方法：

![image-20250116185255943](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250116185255943.png)

使用方法：

OpenFeign已经被SpringCloud自动装配，实现起来非常简单：

1. 引入依赖，包括OpenFeign和负载均衡组件SpringCloudLoadBalancer

   ```xml
   <!--OpenFeign-->
   <dependency>
   	<groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-openfeign</artifactId>
   </dependency>
   <!--负载均衡-->
   <dependency>
   	<groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-loadbalancer</artifactId>
   </dependency>
   ```

   springCloud原本用的是Ribbon而此项目用的是最新的loadbalancer

2. 通过@EnableFeignClients注解，启用OpenFeign功能

   ```java
   @EnableFeignClients
   @SpringBootApplication
   public class CartApplication {}
   ```

3. 编写FeignClient

   ```java
   @FeignClient(value = "item-service")
   public interface ItemClient {
       @GetMapping("/items")
       List<ItemDTO> queryItemByIds(@RequestParam("ids" Collection<Long> ids));
   }
   ```

   

4. 使用FeignClient,实现远程调用

   ```java
   List<ItemDTO> items = itemClient.queryItemByIds(List.of(1,2,3));
   ```

## 连接池

OpenFeign对Http请求做了优雅的伪装，不过其底层发起了http请求，依赖于其它的框架。这些框架可以自己选择，包括以下三种：

- HttpURLConnection: 默认实现，不支持连接池
- Apache HttpClient：支持连接池
- OKHttp支持连接池

OpenFeign整合OKHttp的步骤如下：

1. 引入依赖

   ```xml
   <!--ok-http-->
   <dependency>
   	<groupId>io.github.openfeign</groupId>
       <artifactId>feign-okhttp</artifactId>
   </dependency>
   ```

2. 开启连接池功能

   ```yaml
   feign:
   	okhttp:
   		enabled: true # 开启OKHttp连接池支持
   ```

   

## 日志

OpenFeign 只会在FeignClient所在包的日志级别为DEBUG时，才会输出日志。而且其日志级别有4级：

- **NONE**：不记录任何日志信息，这是默认值。
- **BASIC**：仅记录请求的方法，URL以及响应状态码和执行时间
- **HEADERS**：在BASIC的基础上，额外记录了请求和响应的头信息
- **FULL**：记录所有请求和响应的明细，包括头信息、请求体、元数据

由于Fegin默认的日志级别就是NONE，所以默认我们看不到请求日志

要自定义日志级别需要声明一个类型为Logger.Level的Bean，在其中定义日志级别：

```java
public class DefaultFeignConfig {
    @Bean
    public Logger.Level feignLogLevel(){
        return Logger.Level.FULL;
    }
}
```

但此时这个Bean并未生效，要想配置某个FeignClient的日志，可以在@FeignClient注解中声明：

```java
@FeignClient(value = "item-service", configuration = DefaultFeignConfig.class)
```

如果想要**全局配置**，让所有FeignClient都按照这个日志配置，则需要在@EnableFeignClients注解中声明：

```java
@EnableFeignClients(defaultConfiguration = DefaultFeignConfig.class)
```

# 网关

网关：就是网络的关口，负责请求的路由、转发、身份校验。

![image-20250117224049562](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250117224049562.png)

![image-20250117224600799](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250117224600799.png)

## 路由属性

网关路由对应的Java类型是RouteDefinition,其中常见的属性有：

- id：路由唯一标识
- uri：路由目标地址
- predicates：路由断言，判断请求是否符合当前路由。
- filters：路由过滤器，对请求或响应做特殊处理。

## 路由断言

![image-20250118004509765](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118004509765.png)

## 路由过滤器

网关中提供了33中路由过滤器，每种过滤器都有独特的作用。

![image-20250118004835260](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118004835260.png)

如何统一添加过滤器

```yaml
spring:
	cloud:
		gataway:
			routes:
				- id : item # 路由规则id，自定义，唯一
				  uri: lb://item-service # 路由目标微服务，lb代表负载均衡
				  predicates: # 路由断言，判断请求是否符合规则，符合则路由到目标
				  	- Paht=/items/** # 以请求路径做判断，以/items开头则符合
			default-filters:
				- AddRequestHeader=truth, anyone long-press
				
```

在routes同级中配置默认过滤器即可

# 网关请求处理流程

![image-20250118011627866](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118011627866.png)

步骤一、在网关的登录校验过滤器中，把获取到的用户写入请求头

需求：修改gateway模块中的登录校验拦截器，在校验成功后保存用户到下游请求的请求头中。

提示：要修改转发到微服务的请求，需要用到ServerWebExchange类提供的API，示例如下：

```java
exchange.mutate() // mutate就是对下游请求做更改
    .request(builder -> builder.header("user-info", userInfo))
    .build();
```

步骤二、在hm-common中编写SpringMVC拦截器，获取登录用户

需求：由于每个微服务都可能有获取登录用户的需求，因此我们直接在hm-common模块定义拦截器，这样微服务只需要引入依赖即可生效，无需重复编写

# 配置管理

- 微服务重复配置过的，维护成本高
- 业务配置经常变动，每次修改都要重启服务
- 网关路由配置写死，如果变更要重启网关

![image-20250118231041026](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118231041026.png)

## 配置共享

1. 添加配置到Nacos

   添加一些共享配置到Nacos中

2. 拉取共享配置

   基于NacosConfig拉取共享配置代替微服务的本地配置。

   ![image-20250118232605575](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118232605575.png)

   1. 引入依赖

      ```xml
      <!--nacos配置管理-->
      <dependency>
      	<groupId>com.alibaba.cloud</groupId>
          <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
      </dependency>
      <!--读取bootstrap文件-->
      <dependency>
      	<groupId>org.springframework.cloud</groupId>
          <artifactId>spring-cloud-starter-bootstrap</artifactId>
      </dependency>
      ```

   2. 新建bootstrap.yaml

      ```yaml
      spring:
      	application:
      		name: cart-service # 服务名称
      	profiles:
      		active: dev
      	cloud:
      		nacos:
      			server-addr: 192.168.150.101 # nacos地址
      			shared-configs: # 共享配置
      				- dataId: shared-jdbc.yaml # 共享mybatis配置
      				- dataId: shared-log.yaml # 共享日志配置
      				- dataId: shared-swagger.yaml # 共享日志配置
      ```

   

## 配置热更新

配置热更新：当修改配置文件中的配置时，微服务无需重启即可使配置生效。

前提条件：

1. nacos中要有一个于微服务名有关的配置文件。

   ![image-20250118234348923](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118234348923.png)

2. 微服务中要以特定方式读取需要热更新的配置属性

   ![image-20250118234618623](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250118234618623.png)



## 动态路由

# 雪崩问题

微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是雪崩。

雪崩问题产生的原因是什么？

- 微服务相互调用，服务提供者出现故障或阻塞。
- 服务调用者没有做好异常处理，导致自身故障。
- 调用链中的所有服务级联失败，导致整个集群故障

解决问题的思路有哪些？

- 尽量避免服务出现故障或阻塞
  - 保证代码的健壮性；
  - 保证网络畅通；
  - 能应对较高的并发请求；
- 服务调用者做好远程调用异常的后备方案，避免故障扩撒

## 服务保护方案 -请求限流

请求限流：限制访问微服务的请求的并发量，避免服务因流量激增出现故障。

![image-20250119020040524](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250119020040524.png)

![image-20250119020056308](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250119020056308.png)

## 服务保护方案 -线程隔离

线程隔离：也叫做舱壁模式，模拟船舱隔板的防水原理。通过限定每个业务能使用的线程数量而将故障业务隔离，避免故障扩散。

![image-20250119020430214](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250119020430214.png)

## 服务保护方案 -服务熔断

服务熔断：由断路器统计请求的异常比例或慢调用比例，如果超出阈值则会**熔断**该业务，则拦截该接口的请求。

熔断期间，所有请求快速失败，全都走fallback逻辑。

![image-20250119020943891](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250119020943891.png)

## 服务保护技术

|          | Sentinel                                       | Hystrix                      |
| -------- | ---------------------------------------------- | ---------------------------- |
| 线程隔离 | 信号量隔离                                     | 线程池隔离、信号量隔离       |
| 熔断策略 | 基于慢调用比例异常比例                         | 基于异常比率                 |
| 限流     | 基于 QPS，支持流量整形                         | 有限的支持                   |
| Fallback | 支持                                           | 支持                         |
| 控制台   | 开箱即用，可配置规则、查看秒级监控、机器发现等 | 不完善                       |
| 配置方式 | 基于控制台，重启后失效                         | 基于注解或配置文件，永久生效 |

# Sentinel

Sentinel是阿里巴巴开源的一款微服务流量控制组件。

Sentinel 的使用可以分为两个部分:

- **核心库**（Jar包）：不依赖任何框架/库，能够运行于 Java 8 及以上的版本的运行时环境，同时对 Dubbo / Spring Cloud 等框架也有较好的支持。在项目中引入依赖即可实现服务限流、隔离、熔断等功能。
- **控制台**（Dashboard）：Dashboard 主要负责管理推送规则、监控、管理机器信息等。

将jar包放在任意非中文、不包含特殊字符的目录下，重命名为`sentinel-dashboard.jar`：

然后运行如下命令启动控制台：

```Shell
java -Dserver.port=8090 -Dcsp.sentinel.dashboard.server=localhost:8090 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar
```

## 簇点链路

簇点链路，就是单机调用链路。是一次请求进入服务后经过的每一个被Sentinel监控的资源链。默认Sentinel会监控SpringMVC的每一个Endpoint(http接口)。限流、熔断等都是针对簇点链路中的**资源**设置的。而资源名默认就是接口的青秀区路径

Restful风格的API请求路径一般都相同，这会导致簇点资源名称重复。因此我们要修改配置，把请求方式+请求路径作为簇点资源名称：

## 实现管控

对请求做流控规则-线程数是设置单个线程最大访问数量，QPS是设置

## Fallback

当线路的某个服务不可用时，避免持续的调用失败，直接走一个fallback的方法，以其它方式返回提示数据

1. 将FeignClient作为Sentinel的簇点资源：

   ```yaml
   feign:
   	sentinel:
   		enabled: true
   ```

2. FeignClient的Fallback有两种配置方式：

   - 方式一：FallbackClass，无法对远程调用的异常做处理
   - 方式二：FallbackFactory，可以对远程调用的异常做处理，通常会选择这种

步骤一：自定义类，实现FallbackFactory，编写对某个FeignClient的fallback逻辑：

```java
@Slf4j
public class UserClientFallbackFactory implements FallbackFactory<UserClient> {
    @Override
    public UserClient create(Throwable throwable) {
        // 创建UserClient接口实现类，实现其中的方法，编写失败降级的处理逻辑
        return new UserClient() {
            @Override
            public User findById(Long id) {
                // 记录异常信息，可以返回空或抛出异常
                log.error("查询用户失败", throwable);
                return null;
            }
        }
    }
}
```

步骤二：将刚刚定义的UserClientFallbackFactory注册为一个Bean：

```java
@Bean
public UserClientFallbackFactory userClientFallback(){
    return new UserClientFallbackFactory();
}
```

步骤三：在UserClient接口中使用UserClientFallbackFactory：

```java
@FeignClient(value = "userservice", fallbackFactory = UserClientFallbackFactory.class)

public interface UserClient {
    
    @GetMapping("/user/{id}")
    Useer findById(@PathVariable("id") Long id);
}
```

## 服务熔断

熔断降级是解决雪崩问题的重要手段。思路是由**断路器**统计服务调用的异常比例、慢请求比例，如果超出阈值则会**熔断**该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行该服务的请求。

![image-20250120230311588](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250120230311588.png)

# 分布式事务

下单业务，前端请求首先进入订单服务，创建订单并写入数据库。然后订单服务调用购物车服务和库存服务：

- 购物车服务负责清理购物车信息
- 库存服务负责扣减商品库存

![image-20250121003614625](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121003614625.png)

在分布式系统中，如果一个业务需要多个服务合作完成，而且每一个服务都有事务，多个事务必须同时成功或失败，这样的事务就是**分布式事务**。其中的每个服务的事务就是一个分支事务。整个业务称为一个**全局事务**。

# Seata

Seata是 2019 年  1月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。

官网地址：http://seata.io/,其中的文档、博客提供了大量的使用说明、源码分析。

## 分布式事务解决思路

解决分布式事务，各个子事务之间必须能感知到彼此的事务关系，才能保证状态一致。

![image-20250121004329786](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121004329786.png)

## Seata架构

Seata事务管理中有三个重要的角色：

- TC(Transaction Coordinator) - 事务协调者：维护全局和分支事务的状态，协调全局事务提交或回滚。
- TM(Transaction Manager) - 事务管理器：定义全局事务的范围、开始全局事务、提交或回滚全局事务。
- RM(Resource Manager) - 资源管理器：管理分支事务，与TC交谈以注册分支事务和报告分支事务的状态

![image-20250121010037712](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121010037712.png)

## Seata的使用

1. 首先导入数据库表

2. 导入配置文件到虚拟机中

   ```yaml
   server:
     port: 7099		# Web控制台端口
   
   spring:
     application:
       name: seata-server		# 微服务名称
   
   logging:
     config: classpath:logback-spring.xml	# 日志位置
     file:
       path: ${user.home}/logs/seata
     # extend:
     #   logstash-appender:
     #     destination: 127.0.0.1:4560
     #   kafka-appender:
     #     bootstrap-servers: 127.0.0.1:9092
     #     topic: logback_to_logstash
   
   console:
     user:
       username: admin
       password: admin
   
   seata:
     config:
       # support: nacos, consul, apollo, zk, etcd3
       type: file		# 配置方式 以上是其它可选的方案
       # nacos:
       #   server-addr: nacos:8848
       #   group : "DEFAULT_GROUP"
       #   namespace: ""
       #   dataId: "seataServer.properties"
       #   username: "nacos"
       #   password: "nacos"
     registry:
       # support: nacos, eureka, redis, zk, consul, etcd3, sofa
       type: nacos
       nacos:
         application: seata-server 
         server-addr: nacos:8848 # 需要容器在同一网络下
         group : "DEFAULT_GROUP"
         namespace: ""
         username: "nacos"
         password: "nacos"
   #  server:
   #    service-port: 8091 #If not configured, the default is '${server.port} + 1000'
     security:
       secretKey: SeataSecretKey0c382ef121d778043159209298fd40bf3850a017
       tokenValidityInMilliseconds: 1800000
       ignore:
         urls: /,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/api/v1/auth/login
     server:
       # service-port: 8091 #If not configured, the default is '${server.port} + 1000'
       max-commit-retry-timeout: -1
       max-rollback-retry-timeout: -1
       rollback-retry-timeout-unlock-enable: false
       enable-check-auth: true
       enable-parallel-request-handle: true
       retry-dead-threshold: 130000
       xaer-nota-retry-timeout: 60000
       enableParallelRequestHandle: true
       recovery:
         committing-retry-period: 1000
         async-committing-retry-period: 1000
         rollbacking-retry-period: 1000
         timeout-retry-period: 1000
       undo:
         log-save-days: 7
         log-delete-period: 86400000
       session:
         branch-async-queue-size: 5000 #branch async remove queue size
         enable-branch-async-remove: false #enable to asynchronous remove branchSession
     store:
       # support: file 、 db 、 redis
       mode: db
       session:
         mode: db
       lock:
         mode: db
       db:
         datasource: druid
         db-type: mysql
         driver-class-name: com.mysql.cj.jdbc.Driver
         url: jdbc:mysql://mysql:3307/seata?rewriteBatchedStatements=true&serverTimezone=UTC
         user: root
         password: 123
         min-conn: 10
         max-conn: 100
         global-table: global_table
         branch-table: branch_table
         lock-table: lock_table
         distributed-lock-table: distributed_lock
         query-limit: 1000
         max-wait: 5000
       # redis:
       #   mode: single
       #   database: 0
       #   min-conn: 10
       #   max-conn: 100
       #   password:
       #   max-total: 100
       #   query-limit: 1000
       #   single:
       #     host: 192.168.150.101
       #     port: 6379
     metrics:
       enabled: false
       registry-type: compact
       exporter-list: prometheus
       exporter-prometheus-port: 9898
     transport:
       rpc-tc-request-timeout: 15000
       enable-tc-server-batch-send-response: false
       shutdown:
         wait: 3
       thread-factory:
         boss-thread-prefix: NettyBoss
         worker-thread-prefix: NettyServerNIOWorker
         boss-thread-size: 1
   
   ```

   ![image-20250121013316737](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121013316737.png)

   ![image-20250121013332847](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121013332847.png)

3. `docker network ls`查询各个容器网络端口

   `docker inspect mysql`查询（mysql）容器存在于哪个网络

   `docker logs -f seata` 查看日志 

4. 在虚拟机进行安装在/root目录下执行

   ```shell
   docker run --name seata \
   -p 8099:8099 \		# 微服务和seata的端口
   -p 7099:7099 \		# web 控制台的端口
   -e SEATA_IP=192.168.150.101 \
   -v ./seata:/seata-server/resources \
   --privileged=true \
   --network hm-net \
   -d \
   seataio/seata-server:1.5.2
   ```

## 微服务集成Seata

首先，要在项目中引入Seata依赖：

```xml
<!--统一配置管理-->
  <dependency>
      <groupId>com.alibaba.cloud</groupId>
      <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
  </dependency>
  <!--读取bootstrap文件-->
  <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-bootstrap</artifactId>
  </dependency>
  <!--seata-->
  <dependency>
      <groupId>com.alibaba.cloud</groupId>
      <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
  </dependency>
```

然后，在application.yml中添加配置，让微服务找到TC服务地址：

![image-20250121021521705](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121021521705.png)

## XA模式

XA 规范是 X/Open组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的关系型数据库都对XA规范 提供了支持。Seata的XA模型如下：

![image-20250121024654605](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121024654605.png)

一阶段的工作：

1. RM注册分支事务到TC
2. RM执行分支业务sql但不提交
3. RM报告执行状态到TC

二阶段的工作：

- TC检测各分支事务执行状态

  a.	如果都成功，通知所有RM提交事务

  b.	如果有失败，通知所有RM回滚事务

- RM接收TC指令，提交或回滚事务

XA模式的优点是什么？

- 事务的强一致性，满足ACID原则。
- 常用数据库都支持，实现简单，并且没有代码侵入

XA模型的缺点是什么？

- 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差
- 依赖关系型数据库实现事务

Seata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下：

1. 修改application.yml文件（每个参与事务的微服务），开启XA模式：

   ```yaml
   seata:
   	data-source-proxy-mode: XA # 开启数据源代理的XA模式
   ```

2. 给发起全局事务的入口方法添加@GlobalTransactional注解，本例中是OrderServiceImpl中的create方法：

   ```java
   @Override
   @GlobalTransactional
   public Long createOrder(){
       
   }
   ```

3. 重启服务并测试

## AT模型

Seata主推的是AT模型，AT模型同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。

阶段一RM的工作：

- 注册分支事务
- 记录undo-log（数据快照）
- 执行业务sql并提交
- 报告事务状态

阶段二提交时RM的工作：

- 删除undo-log即可

阶段二回滚时RM的工作：

- 根据undo-log恢复数据到更新前

![image-20250121030056697](C:\Users\29326\AppData\Roaming\Typora\typora-user-images\image-20250121030056697.png)
